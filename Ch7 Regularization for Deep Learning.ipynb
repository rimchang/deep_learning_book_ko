{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch7 Regularization for Deep Learning\n",
    "\n",
    "ch5 에서 일반화와 overfitting에 대해서 다룰때 3가지 상황에 대해 중점적으로 살펴봤습니다\n",
    "- true data gernerating process를 포함하지 않는 경우\n",
    "- true data gernerating process와 일치하는 경우\n",
    "- true data gernerating process를 포함하지만 가능한 다른 process가 많은경우\n",
    "\n",
    "regularization의 목적은 3번째의 경우를 2번째에 가깝도록 만드는 것입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Parameter Norm Penalties\n",
    "\n",
    "보통 파라미터의 norm penalty $\\Omega$를 선택할때 각 레이어의 weight에만 적용하고 bias에는 적용하지 않습니다. bias는 weight보다 적은양의 데이터로도 충분히 정확하게 학습 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.1 L2 Parameter Regularization\n",
    "\n",
    "L2 파라미터 regularization이 있을때의 파라미터 업데이트는 다음과 같습니다.\n",
    "\n",
    "## $ w \\leftarrow  (1-\\epsilon \\alpha)w - \\epsilon \\bigtriangledown _w J(w; X,y)$\n",
    "\n",
    "위의 식을 살펴보면 cost에 대한 gradient 외에도 $(1-\\epsilon \\alpha)w$ 라는 텀을 볼 수 있습니다. 이는 즉 각 step마다 weight update 이전에 일정 상수를 곱해 weight vector를 수축시켜주는 역할을 하게됩니다. 한번의 step마다는 이러한 일이 일어나며 전체 학습 과정에서는 어떤 일이 일어날까요?\n",
    "\n",
    "더 간단한 분석을 위해 objective function을 taylor 급수를 통해 optimal weight에서 2차 근사를 해봅시다. objective function이 실제로 이차형식이고 MSE를 사용하는 linear model 이라고 하면 이러한 근사는 완벽하게 근사 할 수 있습니다. 근사된 cost는 다음과 같습니다.\n",
    "\n",
    "## $\\hat J(\\theta) = J(W*) + \\frac{1}{2}(w-w*)^TH(w-w*)$\n",
    "\n",
    "여기서 살펴볼 것은 이 이차형식 근사에 1차의 텀이 없다는 것입니다. w\\* 이 최소로 정의되었기 때문에 gradient에 관한 텀은 사라지게 됩니다. 비슷하게 w\\*가 J의 minimum point 이므로 H가 positive semidefinite라고 할 수 있습니다.\n",
    "\n",
    "원래는 테일러 급수전개하면 밑의 식인데..\n",
    "$\\hat J(\\theta) = J(W*) + (w-w*)^Tg +  \\frac{1}{2}(w-w*)^TH(w-w*)$\n",
    "자코비안 매트릭스 g가 0이된다??~ optimal W\\*를 가정했으므로... 그래.. 자코비안 g가 0이라는것까지는.. 이해하겠어. 뭐..convex이든...아니든 optimal W\\*에서는 gradient가 0이 되겠지.. 게다가.. linear에 MSE이므로.. convex하기도 하겠다. 그렇다면 왜 H는 positive semidefinite지? (H, g는 정의에 따라.. symetric인건 분명한데!!,, 결국 eigen value가 다 0이상인가? 흠.. convex인데.. opimal value이므로.. 당연히 optimal value에서 downhil즉 양수인건 당연한건가?? 아니 convex이니까 모든 점에서 downhill 인듯)\n",
    "\n",
    "- 4.3.1 Beyond the Gradient: Jacobian and Hessian Matrices : 헤시안 매트릭스는 실수이며 대각행렬이므로 우리는 항상 이를 대각화 할 수 있습니다. 대각화 하여 $d^THd$ 로 분해 할 수 있습니다. 2차 도함수의 특정 방향은 unit vector d에 의해 나타내 지며 이는 헤시안 매트릭스의 고유벡터가 됩니다. d 가 나타내는 방향에 대한 2차 미분의 양은 해당하는 고유값이 나타내가 됩니다. d가 나타내지 않는 방향에 대한 2차 미분은 모든 고유값에 대한 가중 평균이 되게 되며 이때의 가중치들은 0~1의 사이의 값이 되게 됩니다.(왜지?) d와 더 유사한 혹은 더 작은 각을 가지는 vector는 더 많은 가중치를 가지게 됩니다. 가장 큰 고유값은 가장 큰 이차 미분값을 결정하며 가장 작은 고유값은 가장 작은 이차 미분값을 결정합니다. 2차 미분이 양수이면 downhil이라는 뜻이며 음수이면 uphil 0이면 flat이라는 이야기가 된다.\n",
    "\n",
    "\n",
    "## $\\bigtriangledown _w \\hat J(w) = H(w-w*)= 0 $\n",
    "\n",
    "그리고 위의 식 일때 minimum cost function이 되게 됩니다.\n",
    "\n",
    "\n",
    "weight decay의 효과를 살펴보기 위해 위의 식에 gradient의 weight decay 텀을 더해봅시다. 이제 rugularzied version J에 대한 minimum을 찾아야만 합니다. w~라는 변수를 사용하여 regularized version J에 대한 솔루션을 나타내 봅시다. \n",
    "\n",
    "$\\alpha \\tilde{w} + H(\\tilde{w}- w*) = 0\\\\\n",
    "(H + \\alpha I)\\tilde{w} = Hw* \\\\\n",
    "\\tilde{w} = (H + \\alpha I)^{-1}Hw*$\n",
    "\n",
    "알파가 0으로 가까이 갈 수록, regularized solution $\\tilde{w}$ 가 w와 접근하게 됩니다. 만약 알파가 커지게 된다면 어떻게 될까요? H가 실수이며 대각행렬이므로 우리는 $H=QAQ^T$로 대각화 할 수 있습니다. 대각화 한 것을 위의 식에 적용하면 \n",
    " \n",
    "$\\tilde{w} \\\\\n",
    "= (QAQ^T + \\alpha I)^{-1} QAQ^Tw* \\\\\n",
    "= (QAQ^T + \\alpha QQ^T)^{-1}QAQ^Tw* \\,\\,\\, Note \\,\\, I=QQ^T \\\\\n",
    "= [Q(A + \\alpha I)Q^T]^{-1} QAQ^Tw* \\\\\n",
    "= [Q^{T-1}(A + \\alpha I)^{-1}Q^{-1}]QAQ^Tw* \\,\\,\\, Note \\,\\, (AB)^{-1} = B^{-1}A^{-1}\\\\\n",
    "= Q(A + \\alpha I )^{-1} AQ^Tw*$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래는 $\\tilde{w} = QAQ^Tw*$ 이엇던 식이 $Q(A + \\alpha I )^{-1} AQ^Tw*$로 바뀐 것을 볼 수 있습니다. 이는 weight decay가 H의 고유벡터에 의해서 정의된 만큼 w\\*을 rescale하는 효과를 볼 수 있습니다. 특히 H의 고유벡터에 해당하는 $\\tilde{w}$은 $ \\frac{\\lambda_i}{\\lambda_i + \\alpha}$ 만큼 rescale되게 됩니다. \n",
    "\n",
    "H의 고유값이 weight decay보다 상대적으로 클수록($\\lambda_i >> \\alpha$) regularziation 은 상대적으로 작아지게 되며 고유값이 더 작다면 ($\\lambda_i << \\alpha$) 해당 축의 weight는 거의 0으로 수축할 것입니다.\n",
    "\n",
    "- 피규어 7.1 그림에서 보면 첫번째 축의 고유값이 더 작다 (curvate가 작다 즉 완만하다) 즉 수평축으로의 이동은 objective function을 많이 증가시키진 않는다. 이는 objective function이 그쪽으로의 선호를 하지 않는다는 것이며 regularizer는 이 축으로 강한 효과를 가지고 있다. regularizer는 w1을 0에 가깝게 만들 것이다. 두번째 축에서의 이동은 objective function이 매우 민감하게 된다. 이 축에 대한 고유값은 크며 높은 curvature를 나타낸다. 이 결과 weight decay의 w2에 대한 효과는 상대적으로 작게 된다.\n",
    "\n",
    "#### 뭐 하여튼.. 헤시안 매트릭스 의 고유값에 따라 weight decay는 다른 효과를 보인다. 곡률이 크면 weight decay의 효과를 적게 받고 곡류이 작으면 wegiht decay의 효과를 크게 받는다!!\n",
    "\n",
    "파라미터가 objective function을 줄이는게 크게 영향을 끼치는 방향만이 보존되게 됩니다. objective function을 줄이는데 별로 공헌을 하지 않는 방향인, 고유값이 작은 헤시안 매트릭스의 방향은 우리에게 이 방향은 gradient에 큰 영향을 끼치지 않는다는 것을 말해줍니다. 이러한 덜 중요한 방향에 해당하는 weight vector들은 decay 되도록 학습됩니다.\n",
    "\n",
    "지금까지는 일반적인 quadratic cost function에서의 경우를 살펴봤습니다. 실제 머신러닝의 경우인 linear regression의 경우를 살펴보면, L2 regularization이 학습 알고리즘으로 하여금 X의 높은 variance인 부분을 인식할 수 있다고 볼 수 있습니다. 이는 더 낮은 공분산을 가지는 feature를 decay하게 만듭니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.2 L1 Regularization\n",
    "\n",
    "7.20 수식을 살펴보면 L1 REgularization의 효과가 L2와완전히 다르다는 것을 볼 수 있습니다. 특히 regularization 의 효과가 더이상 w_i에 비례하지 않고 단지 sign(w_i) 만이 곱해지게 됩니다. 위의 형태의 gradient에 대한 하나의 결과는 L2 norm에서 했던 것처럼 테일러 급수의 2차 형식의 대수적인 솔루션을 얻을 수 없다는 것입니다.\n",
    "\n",
    "taylor 시리즈를 이용한 이차형식의 cost function의 근사 대신 좀더 정교한 model의 cost를 근사하는 일종의 truncated taylor series라고 생각해 봅시다. 이러한 세팅 안에서는 gradient는 다음과 같습니다. (L1 loss를 넣었더니.. 더이상 objective가 이차형식이 안된다.. 즉  2차로 완벽히 근사를 하지 못함으로.. 이를 그냥 절단된 테일러 급수로 생각하자.. 이는 덜 정교한 근사일 것이다.!!)\n",
    "\n",
    "L1 penalty가 있을땐 헤시안 메트릭스에 대한 명확한 대수적 표현이 불가능함으로 (헤시안이 semi-definite 이다 라는 것 같은). 분석을 위해 헤시안 매트릭스가 대각행렬이라는 가정을 합니다. 이 가정은 linear regeression 문제에서 전처리로 PCA를 사용해 모든 feature간의 상관관계를 제거한 경우로 생각 할 수 있습니다.\n",
    "\n",
    "## $\\hat J(\\theta) = J(W*) + \\frac{1}{2}(w-w*)^TH(w-w*)$\n",
    "\n",
    "여기에 아까랑 똑같이 weight decay의 효과를 살펴보기 위해 위의 식에 L1 term을 더해봅시다. \n",
    "\n",
    "$\\hat J(\\theta) = J(W*) + \\frac{1}{2}(w-w*)^TH(w-w*) + \\alpha ||w||_1$ 이러면 책과 똑같은 식이 나오고 이를 미분해서 optimal value를 구해보면 다음과 같습니다\n",
    "\n",
    "$w_i = sign(w_i^*)max(|w_i^*| - \\frac{\\alpha}{H_{i,i}},0)$\n",
    "\n",
    "$w_i^* >0 $ 인 경우만을 고려해봅시다 이때 두가지 가능성이 있습니다.\n",
    "\n",
    "- $|w_i^*| <= \\frac{\\alpha}{H_{i,i}}$ 인경우 w_i의 opimal value는 w_i = 0 이 됩니다. 이는 i번째 방향에 대해서 $ J(W*) + \\frac{1}{2}(w-w*)^TH(w-w*) + \\alpha ||w||_1$ 에서 앞에텀보다 뒤에텀이 더 크게 되어 L1 regularization이 w_i를 0으로 가깝게 만든 것입니다\n",
    "\n",
    "- $|w_i^*| > \\frac{\\alpha}{H_{i,i}}$ 인 경우 regularization은 optimal value w_i를 0으로 가게 하지 않고 대신 $\\frac{\\alpha}{H_{i,i}}$ 만큼만 shift 시킵니다.\n",
    "\n",
    "$w_i^* <0 $ 인 경우도 비슷하게 L1 penalty는 w_i를 0 혹은 플러스 방향으로 shift만을 시킵니다.\n",
    "\n",
    "L2 regularization과 비교하자면 L1 regularization은 더 sparse한 결과를 갖습니다. L2 regularization을 L1 regularization에서 했던 방법대로 분석하게 되면 optimal w_i\\*가 zero일 때만 w_i가 0 이 되게 됩니다. 이는 L2 패널티는 파라미터가 sparse한 것에 영향을 끼치지 않지만 L1 패널티는 충분히 큰 알파일 경우 파라미터를 sparse하게 만듭니다.\n",
    "\n",
    "섹션 5.61에서 살펴봤듯이 많은 regularization 전략들은 일종의 MAP bayesian inference로 해석할 수 있습니다. L2 패널티의 경우 weight가 Gaussian이라는 prior인 경우와 동일하며 L1 패널티의 경우 weight가 log isotropic laplace distribution인 경우와 동일 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Norm Penalties as Constrained Optimization\n",
    "\n",
    "#### constrained opimization을 할때 보통 라그랑지 multiflier를 사용하여 이를 최적화 한다.\n",
    "\n",
    "\n",
    "### L2, L1 패널티 대신 명시적 제약을 사용하는 경우!!\n",
    "\n",
    "떄때로 이러한 L2, L1과 같은 패널티 대신 명시적인 제약조건을 사용하고 싶을 때가 있다. 4.4에서 설명한대로 SGD를 수정한 알고리즘을 사용하여 gradient update를 하고 이를 feasible point로 다시 reprojection 할 수 있다. 이는 적절한 k 제약조건을 알고 그에 해당하는 알파를 찾는데 시간을 소모하기 싫을때 수용하다.\n",
    "\n",
    "패널티로 제약을 주는 것 대신 이러한 명시적 제약, repojection을 하는 또 다른 이유가 있다. 패널티를 주는 것은 non-convex optimization을 유발할 수 잇으며 이는 local minima에 빠질 수 있다. 뉴럴네트워크를 학습할때 몇몇의 \"dead units\" 들이 보통 나타납니다. 이들은 weight들이 너무 작아서 뉴럴네트워크에 의해 학습된 함수의 행동에 아무런 공헌도 하지 못하는 unit들을 말합니다. norm 패널티를 통해 학습을 라때 이러한 \"dead units\" 이 포함된 상태는 norm of weight를 키우면 cost가 내려감에도 불구하고... 패널티 때문에 local optimal 일 수 있습니다. re-projection을 통해 구현된 명시적인 제약들은 이러한 경우 더 나을 수 있고 weight들을 optimal로 다가가게 할 수 있습니다. re-projection을 통한 명시적 제약은 오직 제약이 걸린 region 밖으로 weight가 나가지 못하도록 하는 제약만을 가지고 있습니다. (반면 L2 패널티는... small weight로 만들려고함)\n",
    "\n",
    "마지막으로 re-projection을 통한 명시적인 제약은 유용 할떄가 많습니다. 이들은 최적화 과정이 안정적이도록 만들 수 있습니다. 높은 lr을 사용할때 큰 weight가 positive feedback 을 만나면 large gradient를 생성해 weight에 대한 매우 큰 update를 초래할 수 있습니다. 만약 이러한 update가 지속되어 weight를 계속적으로 크게 한다면 결국 overflow에 도달하게 될 것입니다. re-projection을 통한 명시적인 제약은 이러한 상한이 없이 weight가 계속적으로 커지는 것을 방지 합니다. Hinton은 이러한 제약조건과 높은 lr을 결합하여 사용하는것을 추천하였고 이는 빠르게 커지는 파라미터 공간에서의 안정성을 높여준다고 합니다.\n",
    "\n",
    "특히 Hinton은 Srebro에 의해 소개된 방법을 추천합니다. weight 매트릭스의 전체의 norm인 Frobenius norm대신 weight 매트릭스의 각 칼럼의 norm에 제약을 거는 방법입니다. weight 매트릭스의 각 열에 대해 제약을 거는 것은 하나의 hidden unit이 매우 큰 weight를 가지는 것을 ㅂ아지 해줍니다. 이를 lagrange function을 사용하여 나타내면 L2 weight decay와 비슷하지만 각각의 hidden unit에 대한 분리된 KKT multiplier가 도출된ㄴ다고 합니다. 이 각각의 KKT multiplier는 각각의 hidden unit들을 분리하여 동적으로 제약을 강요합니다. 실무에서는 이러한 column norm limitation은 re-projection을 통한 명시적인 제약조건으로 구현됩니다.\n",
    "\n",
    "## 요즘엔 이러한 column norm limitation은... 사용되지 않는 것처럼 보인다.. 그냥 이러한 명시적인 제약조건을 사용하는 방법이 있다는 것만 알아두자.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Noise Robustness\n",
    "\n",
    "일부 모델의 경우 작은 분산의 노이즈를 인풋에 추가하는 것은 norm of weight에 패널티를 주는것과 같습니다. 보다 일반적인 경우에 노이즈를 추가하는 것은 L2 패널티를 통해 파라미터를 줄이는 것보다 더 강력합니다. 특히 hidden units에 노이즈를 추가 할때 더 강력합니다. \n",
    "\n",
    "또 다른 노이즈를 추가하는 방법은 하나는 weight에 노이즈를 추가하는 것입니다. 이러한 기술은 RNN에서 많이 사용되었습니다. 이는 일종의 weight에 대한 stochastic implementation of bayesian inference로 볼 수 있습니다. 베이지안은 학습을 model weight에 불확실성이 있다고 생각하며 이러한 불확실성을 나타내기 위해 확률 분포를 사용합니다. \n",
    "\n",
    "MLP에서 weight 노이즈를 추가 하는 것은 파라미터가 weight perturbation에 output이 민감하지 않은 파라미터를 찾도록 한다. 이는 model이 weight에 변화에 민감하지 않도록 강제하며 local minima가 아닌 flat region에 둘러 쌓인 minima를 찾도록 도와준다.\n",
    "\n",
    "### Noise 추가 방법\n",
    "- input noise : 일종의 norm penalty\n",
    "- hidden units \n",
    "- weight : 일종의 stochastic implementation of bayesian inference\n",
    "\n",
    "## 7.5.1 Injecting Noise at the Output Targets\n",
    "\n",
    "대부분의 데이터 셋은 약간의 label miss를 포함하고 있습니다. y가 잘못되어 있다면 MLE를 하는데 해로울 수 있습니다. 이를 명시적으로 방지하는 하나의 방법은 label에 농지르르 추가하는 것입니다. 예를 들어 어떤 상수 epsilon에 대해 train set label이 1-epsilon의 확률로 정확하다고 가정하며 epsilon만큼은 부정확하다고 가정합니다. 이러한 가정은 cost function을 해석적으로 해석할 수 있게 해줍니다. 예를 들어 label smoothing 은 softmax target인 0, 1을 $\\frac{\\epsilon}{k}, 1-\\epsilon$ 으로 대체하고 표준적인 Cross entropy를 이용하여 학습합니다. softmax classfier, hard target을 사용하는 MLE는 수렴하지 못하는 경우가 생길 수 있습니다. softmax가 절대로 0, 1 이라고 예측을 하지 못합니다. 그래서 이는 weight를 계속적으로 크게 만들어 예측을 극도로 만듭니다. 이는 weight decay와 같은 방법을 통해 방지 할 수 있습니다. Label smoothing은 correct classfication을 방해하지 않으면서 hard prob으로 가는 것을 방지하는 효과가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9 Parameter Tying and Parameter Sharing\n",
    "\n",
    "<b>parameter sharing</b> :  한 모델의 파라미터가 다른 모델과 같도록 하는 것 이는 메모리를 매우 절약해주며 파라미터를 줄여줘 regularization의 효과가 있다. ex) CNN\n",
    "\n",
    "## 7.10 Sparse Representations\n",
    "\n",
    "7.46 은 sparse 파리미터화된 linear regression model을 보여주며 7.47은 데이터 x에 대한 sparse representation linear regession을 보여준다. \n",
    "\n",
    "<b> orthogonal matching pursuit</b> : activation element중 non-zero element가 k개가 넘지 않도록 하는 제약조건, 이 제약조건을 가진 최적화 문제는  weight가 직교하도록 함으로써 효율적으로 풀 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
