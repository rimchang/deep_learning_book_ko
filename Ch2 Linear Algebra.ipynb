{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frobenius norm\n",
    "\n",
    "매트릭스에 대한 norm을 정의할 필요가 있을 때가 있다. 매트릭스의 크기를 재기위한 norm중 하나로 Frobenius norm이 있다.\n",
    "\n",
    "## $||A||_F = \\sqrt{\\sum_{i,j}A^2_{i,j}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 The Trace Operator\n",
    "\n",
    "Trace는 다음과 같이 정의된다. \n",
    "\n",
    "## $Tr(A) = \\sum_i A_{i,j}$\n",
    "\n",
    "#### Props\n",
    "\n",
    "- ## $||A||_F = \\sqrt{Tr(AA^T)}$\n",
    "- ## $Tr(A) = Tr(A^T)$\n",
    "- ## $Tr(ABC) = Tr(CAB) = Tr(BCA)$ 곱한 사상이 정사각행렬인 trace는 교환법칙이 성립\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 The Determinant\n",
    "\n",
    "행렬식은 eigenvalue의 모든 곱과 같습니다. 행렬식의 절대값은 일종의 선형사상이 space를 얼마나 팽창하거나 수축하는지를 나타냅니다. (일종의 power? volume?이라고 말하는 사람도 봤다) 행렬식이 0이라면 매트릭스곱으로 나타나는 공간이 적어도 한 차원이상 수축하게 되며 volume을 잃게 됩니다. 행렬식이 1이라면 volume을 보존하게 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 Example: Principal Components Analysis\n",
    "\n",
    "let m point $x^{(1)} ,\\cdots x^{(m)}$ 이 포인트들을 lossy compression을 하고 싶다고 하자. lossy compression의 의미는 이들 point들을 적은 메모리로 표현하지만 정보를 잃을 수 있다는 것이다. 최대한 적은 정보를 잃는 것을 원한다고하자.\n",
    "\n",
    "input을 어떤 low dimensional code vector c로 나타내는 $f(x)=c$ 라는 함수가 있고 encoding vector를 다시 decoding하는 $g(f(x)) = g(c) \\approx x $ 라는 decoding function이 있다고 하자.\n",
    "\n",
    "<b>decoding function을 선택함에 따라 PCA가 정의된다. </b> decoder를 매우 간단하게 하기 위해 간단한 매트릭스 곱으로 decoding function을 정의하자. 그렇다면 $f(c) = Dc$ 라는 꼴이 된다. \n",
    "\n",
    "$f(c) = Dc$ 이러한 decoder를 위해 optimal code를 계산하는 것은 매우 어렵다. 문제를 좀더 쉽게하기 위해 PCA는 D 의 칼럼이 서로 직교하도록 제약을 둔다. (D가 l=n이 아닌이상 여전히 직교행렬이라고는 할 수 없다)\n",
    "\n",
    "지금까지의 제약으로만으로는 infinity solution이 존재한다. 유일한 해를 찾기 위해 D의 칼럼벡터들이 unit norm 이라는 제약을 둔다.\n",
    "\n",
    "이러한 아이디어를 구현하기 위해선 optimal code c\\*를 어떻게 구할지에 대한 계산이 필요하다. 하나의 방법은 reconstruction error를 줄이는 것이다. PCA에서는 L2 norm을 사용하여 reconstruction error를 줄인다.\n",
    "\n",
    "## $c* = argmin_c ||x-g(c)||_2$\n",
    "\n",
    "L2 norm의 제곱을 써도 되고 제곱안해서 써도 되는데 두 경우 모두 같은 양의 c의 값을 줄이게 된다. L2 norm은 non-negative이며 제곱연산은 monotically increasing이므로 같은 양의 c의 값을 줄이게 된다.\n",
    "\n",
    "## $c* = argmin_c ||x-g(c)||_2^2$\n",
    "\n",
    "쭈주주죽 해서 optimal c를 구했음!! 이제 해야할것은 매트릭스 D를 구하는 것!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
